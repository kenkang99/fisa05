### Spark란?
- 대규모 데이터 처리에 사용되는 신속하고 보편적인 엔진

- Java나 Scala, Python같은 실제 프로그래밍 언어를 사용해 스크립트를 작성할 수 있는 유연성을 제공하고 복잡한 데이터를 조작이나 변형, 분석이 가능(머신 러닝이나 데이터 마이닝, 그래프 분석, 데이터 스트리밍 등)

- 드라이버 프로그램 보유
    - 드라이버 프로그램이란?
        - 작업을 어떻게 진행할지 통제하는 스크립트
        - 클러스터 관리자(Hadoop, Mesos 등)을 통해 수행
    - 범용 컴퓨터 클러스터에 작업을 분배 후 병렬 처리
    - 집행자 프로세스
        - 캐시와 작업(task)를가짐
            - 캐시는 Spark 성능의 키
    - 방향성 비사이클 그래프로도 성능의 키가 된다(빠르다)

- 회복성 분산 데이터(RDD) 를 사용
    - 데이터 세트를 나타내는 객체
    - RDD 객체에 다양한 함수를 사용해 이를 변형하거나 리듀싱하고 분석해서 새로운 RDD를 생성가능 (보통 입력 데이터의 RDD를 가져와 변형하는 스크립트를 작성)
    - RDD에 프로그래밍을 하면 spark 위에 구축된 라이브러리 패키지 함께 사용가능

- MLLib 라이브러리 사용가능
    - ML or data mining 라이브러리

- Scala를 보통 씀
    - 훨씬 빠르고 안정적인 성능을 갖고 있는데 이는 Python처럼 오버헤드가 필요 없기 때문(스칼라는 Java 바이트코드에서 바로 컴파일하기 때문)

#### RDD란?
RDD는 회복성 분산 데이터 세트 (Resilient Distributed Dataset)의 약자로 Spark 내부에서 일어나는 지저분한 일들을 추상적으로 표현한 것

- 작업을 클러스터에 고르게 분산하고 실패가 생겨도 회복할 수 있으며 사용자에게 데이터 세트처럼 보임

- RDD 객체는 키-값 정보나 일반적인 정보를 저장하는 수단이고
클러스터에 알아서 작업함

#### RDD 만드는 법
- 보통은 RDD를 어떤 텍스트 파일에서 불러옵니다

- 드라이버 프로그램이 SparkContext라는 것을 만듬

    - 예를 들어 'nums'라는 RDD를 만들고 싶다면 1, 2, 3, 4의 배열을 가지고 'parallelize'를 호출합니다

    - 그러면 1, 2, 3, 4 값을 가진 RDD를 생성하고 그 RDD를 사용해 다양한 작업을 할 수 있죠

    - 이 외 HiveContext,JDBC, Cassandra, HBase,Elasticsearch 등과 연결된 데이터베이스나 JSON, CSV, 서열 파일, 객체 파일, 압축된 양식 등 Hadoop이 지원하는 실제 구조화된 데이터 파일을 사용해 만들 수 있음

#### RDD로 할 수 있는 일

기본적으로 MapReduce에서 했던 매핑과 리듀싱을 할 수 있습니다

RDD에 'map'을 사용하여 각 입력 행에 어떤 함수를 적용해

전환된 데이터를 가지고 새로운 RDD를 만듭니다