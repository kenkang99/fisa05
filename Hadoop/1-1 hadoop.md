hive라는 툴로 query해서 데이터를 조회할 수 있다

```sql
SELECT movie_id, count(movie_id)
FROM ratings
GROUP BY movie_id
ORDER BY count(movie_id) -- hive에서는 집계함수를 order by에 넣는 것을 지원하지 않음 -> 별칭으로 사용해줘야함
DESC; 
```

##### hadoop의 정의
hortonworks( hadoop 플랫폼 제공사) 의 hadoop 정의 - 범용 하드웨어로 구축된 컴퓨터 클러스터의 방대한 데이터 세트를 분산해 저장하고 처리하는 오픈 소스 소프트웨어 플랫폼
다수의 pc를 활용해 빅데이터를 다룬다

대화식 쿼리가 가능해짐

##### hadoop의 장점

1. 분산 저장의 장점은 클러스터에 컴퓨터를 더하기만 하면 그 컴퓨터의 하드 드라이브가 데이터 저장소의 일부가 된다는 것
-> Hadoop은 클러스터의 모든 하드 드라이브에 걸쳐 분산돼 있는 모든 데이터를 단일 파일 시스템으로 보여줌

2. 데이터의 여분도 제공
-> 클러스터의 컴퓨터 중 하나에 불이 나도 데이터의 백업 복사본을 클러스터의 다른 컴퓨터에도 보관하기 때문에 데이터 복구가 가능해짐

3. 병렬처리로 속도가 빠름
-> 클러스터 내 모든 컴퓨터 CPU에게 작업을 분배하여 동시에 처리하게 함

4. Hadoop 클러스터는 선형이라 수평적으로 확장
-> 더 많은 데이터를 다루거나 더 빨리 처리해야 한다면 클러스터에 컴퓨터를 추가한 만큼 더 빨라짐



![image](https://ducmanhphan.github.io/img/hadoop/architecture/core-hadoop-system.png)

분홍색은 hadoop 자체의 부분
나머지는 add-on

##### hadoop의 메인 3가지 요소

1. HDFS
- 클러스터의 하드 드라이브들을 하나의 거대한 파일 시스템으로 사용하는 것(Hadoop의 '분산 데이터 저장소' 역할을 담당)
- 구글파일시스템(GFS) 에서 발전돼 탄생

2. Yarn
+ 'Yet Another Resource Negotiator'의 약어, '또 다른 리소스 교섭자'라는 뜻
+ 데이터 처리 부분을 담당, 컴퓨터 클러스터의 리소스를 관리하는 시스템
    + 누가 작업을 언제 실행하고 어떤 노드가 추가 작업을 할 수 있고, 누구는 할 수 없고 등을 결정 ( 리소스 교섭자 역할)
+ 부가 애플리케이션으로 대표적으로 **Mapreduce** 존재

3. Mapreduce
    + 데이터를 클러스터 전체에 걸쳐 처리하도록 하는 프로그래밍 메타포 혹은 프로그래밍 모델
    + Mapreduce 구성
        1. mapper
            - 클러스터에 분산돼있는 데이터를 효율적으로 동시에 변형시킨다

        2. reducer
            - mapper에서 변형된 데이터를 집계시킨다

    + 원래 yarn 내부에 Mapreduce가 존재했는데 이제는 따로 분리돼 yarn 위에 존재하게 됨

##### hadoop 생태계 메인 외 요소들

1-1. pig
+ mapreduce 위에 존재하는 고수준 스크립팅 언어
+ Pig는 고수준의 API'로써 많은 경우 SQL과 비슷한 간단한 스크립트를 작성해 쿼리를 연결하고 복잡한 답을 구함
+ 작성된 스크립트를 MapReduce가 읽을 수 있도록 번역하고 MapReduce는 다시 YARN과 HDFS에게 데이터를 처리하고 원하는 답을 가져옴

1-2. hive
+ mapreduce 위에 존재하는 pig보다 더 sql 데이터베이스처럼 생긴 프로그램
+ SQL 쿼리를 받고 파일 시스템에 분산된 데이터를 SQL 데이터베이스처럼 취급
+ 셸 클라이언트나 ODBC* 등을 통해 *Open Database Connectivity 데이터베이스에 접속가능

2. apache ambari
+ 모든 것 위에 존재하는 프로그램, 클러스터 전체
+ 클러스터에서 어떤 시스템을 사용하고 얼마나 많은 리소스를 사용하는지 등 무슨 일이 일어나는지 시각화해서 보여주고
+ Hive나 Pig 쿼리를 실행하거나 데이터베이스를 불러올 수 있음
+ 이 기능을 하는 다른 기술도 있긴한데 hortonworks는 ambari를 사용한다 


##### hadoop 외 차선책 소프트웨어들

1. mesos
- yarn과 비슷한 기능함(리소스 관리)
- hadoop에서 spark랑 연동이 가능하게 됨

##### hadoop 생태계 내 부가적인 소프트웨어들

1-1. spark
- mapreduce와 동일선상
- 같은 기능 중 클러스터의 데이터를 신속하고 효율적이며 안정적으로 처리가능
- YARN이나 Mesos 중 어느 쪽을 기반으로 하든 데이터에 쿼리 실행 가능
- spark 스크립트 작성 시 python, java, scala를 주로 사용, scala가 제일 추천됨

1-2. tez 
- spark와 동일선상인데 **방향성 비사이클 그래프**를 사용
- Hive와 함께 사용되어 성능을 가속

2. Hbase
- 클러스터의 데이터를 트랜잭션 플랫폼으로 노출하는 역할을 하는 NoSQL 데이터베이스
- 단위 시간당 실행되는 트랜잭션의 수가 큰 아주 빠른 데이터베이스
- OLTP 트랜잭션을 하는데 적합
- 클러스터에 저장된 데이터(mapreduced 된 데이터일 수 있음)를 노출-> 다른 시스템에 노출

3.  Apache STORM
- 스트리밍 데이터를 처리
- 센서나 웹로그로부터 데이터를 STORM이나 'Spark Streaming'을 통해 실시간으로 처리
- 데이터가 실시간으로 입력됨에 따라 실시간으로 '기계 학습'을 업데이트하거나 데이터를 데이터베이스에 저장

4. oozie
- 클러스터의 작업을 스케줄링
- 데이터를 Hive에 불러와서 Pig를 통해 통합하고 Spark를 통해 쿼리한 후에 결과를 HBASE로 변환하는 것을 oozie로 관리 및 실행 가능

5. zookeeper
-  클러스터의 모든 것을 조직화
- 어떤 노드가 살아있는지 추적 가능
- 클러스터의 공유 상태를 안정적으로 확인 가능

##### 데이터수집에 특화된 시스템
어떻게 외부 데이터를 클러스터와 HDFS로 가져올 수 있을까?

1. Sqoop
- Hadoop의 데이터베이스를 관계형 데이터베이스로 엮어냄( RDBMS에서 데이터 가져오기 & 내보내기 둘다 가능)

- ODBC나 JDBC로 소통 가능한 데이터를 HDFS의 파일로 변형가능 	( [ODBC,JDBC란?](#알쓸신잡) )

- 레거시 데이터베이스(유지보수 안되는, 옛 데이터)와 Hadoop을 잇는 연결 장치

- 

2. Flume
- 대규모 웹로그를 안정적으로 클러스터에 불러오기

- 실시간으로 웹 서버의 웹로그를 감시하고 클러스터에 게시해 STORM이나 Spark Streaming을 사용해 처리

3. kafka
- PC 혹은 웹 서버 클러스터에서 모든 종류의 데이터를 수집해 Hadoop 클러스터로 내보냄


4. 이외에 데이터수집 시스템
- mysql, cassandra, mongodb
    -  Cassandra와 MongoDB는 HBase처럼 '기둥형 데이터 스토어'이고 웹 애플리케이션 등에 데이터를 실시간으로 노출하는데 활용가능 -> 실시간 애플리케이션과 클러스터 사이에 Cassandra나 MongoDB 같은 층을 만들어두는 것을 추천 ( [기둥형 데이터 스토어란?](#알쓸신잡) )

##### 쿼리 엔진
쿼리를 실행하고 코드를 작성할 필요 없이 클러스터에서
어떤 의미를 추출하는 작업해주는 엔진

- hive
- apache drill
    - 다양한 NoSQL 데이터베이스에 SQL 쿼리를 작성해 사용가능하게끔 함
    - Hbase, Cassandra 혹은 MongoDB의 데이터베이스와 소통 가능 & 소통한 내용을 엮어서 이질적인 데이터 스토어들에 걸쳐 쿼리를 작성하고 종합이 가능

- HUE
    - Hive, HBase에 잘 작동하는 쿼리를 **대화형**으로 생성 가능

- Apache PHOENIX
    - Apache DRILL과 비슷, 전체 데이터 스토리지 기술에 걸쳐 SQL 쿼리 가능 & **ACID를 보장 및 OLTP 제공**

- presto
    - 전체 클러스터에 쿼리를 실행 가능

- zeppelin
    - 클러스터와의 상호작용과 사용자 인터페이스를 노트북 유형으로 접근시킴


###### 알쓸신잡
+ ODBC나 JDBC로 소통 가능한 데이터란?
    +  관계형 db(ex - mysql)를 외부 툴(ex- vscode)에서 ODBC/JDBC 드라이버를 통해 DB를 접속하여 외부 툴에서 관계형 db의 데이터를 사용가능
    + **nosql같은 db는 연결불가능**

| 구분                                    | 설명                                        |
| ------------------------------------- | ----------------------------------------- |
| **ODBC (Open Database Connectivity)** | 마이크로소프트가 만든 **데이터베이스 연결 표준 API** (C언어 기반) |
| **JDBC (Java Database Connectivity)** | 자바에서 DB에 연결하기 위한 **표준 API** (Java 기반)     |



+ 기둥형 데이터 스토어란?
    + 행 또는 테이블별로 저장하는 RDBMS와 달리 데이터를 컬럼별로 저장하는 구조를 띈 DBMS( 보통 nosql 구조임)
    + 수평 확장 구조라 실시간 대규모 처리에 강한 구조임
        + MySQL은 행 중심이라 트랜잭션, 복잡한 JOIN, 정합성 유지에 강점
        + 기둥형은 유연한 컬럼 관리 + 대규모 읽기 처리 + 분산 처리에 강점 ( 칼럼별 분산처리는 값만 다르지 외적인 것들은 모두 동일한 환경이니까(ex- 같은 칼럼의 자료형은 동일 ))

    + 행지향 DB는 왜 수평 분산에 약할까?
        + 데이터 정합성 보장(join 등)하는데 하나의 트랜잭션이 여러 노드에 걸쳐있으면 락과 동기화 비용이 급증하기 때문
